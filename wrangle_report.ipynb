{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wrangle_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering\n",
    "For this project, we were expected to gather data from 3 different sources, each with a unique way of acquiring the data.\n",
    "\n",
    "-  **WeRateDogs Twitter archive data (twitter_archive_enhanced.csv)**:<br>\n",
    "This dataset was made available by the Udacity team as a csv file. For this data wrangling project, I imported the pandas library and went ahead to read the csv file, using the \"**pd.read_csv('filename.csv')**\", into a dataframe called \"**df_twitter_arch**\".\n",
    "\n",
    "\n",
    "-  **Image Predictions data (image_predictions.tsv)**:<br>\n",
    "We were expected to programmatically download this dataset from a link provided using the **requests** library. \n",
    "To do this I imported the request library and then used the get method to download the tsv file from the link.\n",
    "\n",
    "> *import requests*<br>\n",
    "> *res = requests.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv')*<br>\n",
    "\n",
    "Next, I read the content of the downloaded file into a dataframe (df_image_predictions) using the read.csv() method. \n",
    "\n",
    ">*with open(\"image_predictions.tsv\", mode='wb') as file:*<br>\n",
    "       > *file.write(res.content)*<br>\n",
    ">*df_image_predictions = pd.read_csv('image_predictions.tsv', sep = '\\t')*<br>\n",
    "\n",
    "-  **Additional data from the Twitter API (tweet_json.txt)**:<br>\n",
    "To query the additional data via API, I imported and used the tweepy and json libraries. I proceeded to authenticate the necessary credentials (API Key, API Key Secret, Access Token and Access Token Secret) and ensured that the rate limit was duly observed. <br>'\n",
    "> twit_api = tweepy.API(authenticate, wait_on_rate_limit=True)\n",
    "\n",
    "\n",
    "Following this stage I created the 'tweet_json.txt' file and wrote the data fetched via API to it. Having sucessfully created the file I then proceeded to read it into a pandas dataframe using a loop. As part of the project, we were only expected to load the 'tweet_id', 'retweet_count' and 'favorite_count' columns into the datframe, which was done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Data\n",
    "\n",
    "Data assessment was done both visually and programatically.<br> Though there are numerous issues within the datasets to be cleaned the below sections list the issues noted and addressed for this project.\n",
    "\n",
    "### Visual Assessment\n",
    "Within the jupyter notebook, and excel for the twitter_archive_enhanced.csv file, I viewed the 3 datasets, taking note of some quality and tidiness issues: including missing values, inaccurate data, etc\n",
    "\n",
    "Below are some of the issues detected from the visual assessment:\n",
    "-  The dog stages in the twitter archive dataset have been split into separate columns instead of being one column with a dog's stage indicated<br>\n",
    "\n",
    "\n",
    "-  df_twitter_arch (twitter archive dataset) has Html tags in the source column<br>\n",
    "\n",
    "\n",
    "- Some of the values in the doggo, floofer, pupper and puppo were indicated as 'None' instead of being null values<br>\n",
    "\n",
    "\n",
    "- Some column headers in the image predictions dataframe cannot be easily understood.\n",
    "\n",
    "\n",
    "### Programmatic Assessment\n",
    "The pandas info and describe methods were used for the assessment. Below were some of the issues detected:\n",
    "- df_twitter_arch (twitter archive dataset) - in_reply_to_status_id and in_reply_to_user_id columns have numerous null values <br> \n",
    "\n",
    "\n",
    "- Timestamp column in the df_twitter_arch (twitter archive dataset) is a string instead of a datetime column<br>\n",
    "\n",
    "\n",
    "- The favorite_count and retweet_count columns in the df_twt_apidata frame are floats instead of int<br>\n",
    "\n",
    "\n",
    "\n",
    "- For some of the tweet ids in the twitter archive dataset there are no corresponding data values in the dataset retrieved via the Twitter API or the image prediction dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "Before cleaning the various datasets, I made copies of each to serve as back up. For each of the issues observed I used the necessary code to correct it using the Define, Code and Test procedure. Where necessary columns or rows in the dataset was dropped and in some cases the vaues in the dataset were adjusted.<br>\n",
    "\n",
    "- The datatypes for the Timestamp, tweet_id and ratings columns were converted to datetime, string and float dataypes respectively.\n",
    "\n",
    "- Retweets and retweet related columns were removed from the twitter archive dataframe\n",
    "\n",
    "- The dog stage columns; doggo, floofer, pupper and puppo, were merged into one column called dog_stage.\n",
    "\n",
    "- HTML tags in the source column (twitter archive dataframe) were removed.\n",
    "\n",
    "- Column headers for the image prediction dataframe were changed to be make it easier to understand.\n",
    "\n",
    "- Finally, following cleaning of the respective issues obeserved, the 3 datasets were merged into one dataset, 'twitter_archive_master.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
